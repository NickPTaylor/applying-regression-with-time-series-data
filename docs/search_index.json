[
["index.html", "Applying Regression with Time Series Data Abstract", " Applying Regression with Time Series Data Nick P Taylor April 20, 2017 Abstract In this report, I will demonstrate how regression analysis can be accurately applied in the case that the explanatory variables and the response variable have been collected as a time series. For simplicity, I consider simple models of the form \\[ y=x+e \\] where \\(y\\) is a response to be predicted, \\(x\\) is an explanatory variable and \\(e\\) is an error term. If \\(e\\) is independent and identically distributed (iid) and is normally distributed, then it is appropriate to apply a simple linear regression model to estimate a relationship, \\(y=x+e\\). However, applying a simple linear regression model to time series data will result in inaccuracies. Time series typically exhibit auto-correlation in the response and the explanatory variable which are not correctly accounted for in the simple linear regression model. Application of such a model will be inefficient which means that the predicted confidence intervals for the coefficients will be incorrect. ARIMAX is a modelling approach that can be used to extend linear regression to correctly account for the auto-correlation. In this report, simulations will be used to demonstrate the application of ARIMAX modelling using the R programming language. The validity of this model applied to time series data will be demonstrated, as will the invalidity of simple regression models applied to the same data. "],
["introduction.html", "1 Introduction", " 1 Introduction Recently, interest has emerged in performing predictive analyses in real-time in order to predict the operating outputs of subsystems of a power plant. Being able to perform such predictions with confidence enables the ability to forecast the impending operation for a range for scenarios and to take suitable actions to optimise overall power plant operation. In statistical analysis, confidence is formally quantified by confidence intervals. Computing the confidence interval accurately is an essential aspect of developing models that are useful and trustworthy. However, time series data poses challenges and requires special treatment. Failure to account for time series effects in data will result in inaccurate predictions of confidence intervals and a model that is not as useful as it could be. Rob J. Hyndman and Athanasopoulos (2014) explain in §1.4 that most forecasting data can broadly be classified as: Cross-sectional. The training data are taken from a random cross-section of a population. The aim is to predict the value of something that has not been observed, using the information on the cases that have been observed. In this report, the variable to be predicted will be referred to as the reponse and the variable(s) that are used to compute the prediction will be referred to as explanatory variable(s). Time series. The training data are collected chronologically. The objective of the model is typically to predict the response at some time in the future. This is achieved by attempting to explain variance in the response by the variance in the response that has previously occurred. For example, time series may exhibit annual seasonality so one can attempt to forecast a variable by using information pertaining to the response of that same variable last year. Also, it is common that data in time series will be dependent on the data that has recently occurred. This is known as auto-regression. When using a time series sample, often the aim is to predict a response from its own history. In §9.1, Rob J. Hyndman and Athanasopoulos (2014) discuss a more complex scenario: Dynamic Regression. Similarly to cross-section data, the aim is to predict a response from explanatory variables. However, the data is not observed as a cross-section but rather as a time series. As such, the model is a hybrid of the cross-sectional and time series forecast classifications. Modelling using this type of data presents specific challenges. Consider the following question which may arise in the context of combustion in a power plant: Can some response, e.g. NOx emissions, be predicted given one or more explanatory variables e.g. load, fuel composition, stochiometry, etc. Often, the data available to answer such questions is in the form of a time series, for example, data from a test rig or a plant DCS (Dynamic Control System). Hence, the classification of the appropriate model is the dynamic regression. The technical basis for the methods used in this report are described in Rob J. Hyndman and Athanasopoulos (2014) which is freely available online book. The focus in this report is to demonstrate the methods and their adequacy through simulation. The R programming language will be used; this report is written with the bookdown package (Xie 2016). The analysis in this report is intended for readers who have some knowledge of statistics. Readers are assumed to be comfortable with terminology such as: linear regression, expected value, variance, standard error, irreducible error, normal distribution, confidence interval, bias, statistical significance testing, p-values, auto-correlation, etc. References "],
["method.html", "2 Method 2.1 Setup 2.2 R Packages 2.3 Simulations 2.4 Generic Modelling Function", " 2 Method 2.1 Setup This block of code is for basic setup of the analysis described in this report. # require the following libraries ---------------------------------------------- library(tidyverse) ## Loading tidyverse: ggplot2 ## Loading tidyverse: tibble ## Loading tidyverse: tidyr ## Loading tidyverse: readr ## Loading tidyverse: purrr ## Loading tidyverse: dplyr ## Conflicts with tidy packages ---------------------------------------------- ## filter(): dplyr, stats ## lag(): dplyr, stats library(broom) library(stringr) library(forecast) library(multidplyr) library(stringr) # setup knitr options ---------------------------------------------------------- knitr::opts_chunk$set( comment = &quot;##&gt;&quot;, collapse = TRUE, fig.width = 6, fig.asp = 0.618, out.width = &quot;70%&quot;, fig.align = &quot;center&quot; ) 2.2 R Packages In general, tidyverse packages are used for manipulating data and applying functions across data frames (Wickham 2017). Plots in this report use ggplot (Wickham 2009), which is imported as part of the tidyverse. The tidyverse packages are complimented by the broom package for tidying model classes (Robinson 2016). ARIMAX modelling and ggplot style time series plotting is made available through the forecast package (Rob J Hyndman 2017). Since modelling is performed for many simulations, significant time savings are available by parallelising the computations. In order to do this, the multidplyr package is used; this is currently only available as a development package on GitHub (Wickham, n.d.). 2.3 Simulations Observational data will be modelled by simulation. The advantage of using simulation data is that the population characteristics are known so it is possible to quantify the performance of a statistical model. Two simulations will be generated: A cross-sectional sample with one response and one explanatory variable. A time series sample with one response and one explanatory variable. The performance of simulations will be assessed by: Bias - the accuracy of the predicted coefficient of the explanatory variable. Efficiency - the accuracy of the predicted confidence interval. The next block of code provides basic setup of the simulations: set.seed(20170410) no_sims &lt;- 10000L # L is for integer! no_obs &lt;- 500L In each simulation case, 10000 simulations will be generated with 500 observations in each simulation. A seed has been set so that the simulations are reproducible. 2.3.1 Cross-sectional Simulation To simulate a cross-sectional data sample, the data will be generated according to \\[ y = x + e \\tag{2.1} \\] where \\(e\\) is an irreducible error term; errors will be independent and identically distributed (iid) random variables from a standard normal distribution i.e. mean and variance are 0 and 1 respectively. \\(x\\) will be sampled from a uniform distribution bounded between -10 and 10. In this model, \\(x\\) varies with \\(y\\) and the coefficient is 1. The model has deliberately been made very simple. Of course, in reality, the statistician will not know the form of the model and the coefficients, neither will the irreducible error, \\(e\\), ever observed. Statistical methods enable an estimate of the coefficients and error in the overall population to be inferred from a sample of observations. This code block creates 10000 randomly generated simulations of sample size 500 of cross-sectional observations: # function for making simulation IDs ------------------------------------------- make_id &lt;- function(x) { str_c(&quot;sim_&quot;, str_pad(x, width = 3, side = &quot;left&quot;, pad = &quot;0&quot;)) } # function to generate simulation ---------------------------------------------- sim_xs_obs &lt;- function() { tibble( # order of observation -------------------------------------------------- t = 1:no_obs, # explanatory variable -------------------------------------------------- x = runif(no_obs, -10, 10), # response -------------------------------------------------------------- y = x + rnorm(no_obs) ) } # repeat simulation many times ------------------------------------------------- sims_xs &lt;- tibble( id = map_chr(1:no_sims, make_id), obs = replicate(no_sims, sim_xs_obs(), simplify = FALSE)) Note that the variable \\(t\\) in the above nominally refers to time. This is for consistency with the time series simulation, described in §2.3.2. In the case of cross-sectional samples, time is not relevant and \\(t\\) instead refers to order in which data are observed. Next, functions are defined and invoked to generate two plots: A scatter plot of the response, \\(y\\), versus the explanatory variable, \\(x\\), for the simulation. This includes a black dashed line of \\(y=x\\) i.e. the known population relationship. A scatter plot of the explanatory variable, \\(x\\), in order of when it is observed i.e. its index. Note that only the middle 100 observations from the first 2 simulations are considered to enhance clarity. gg_sims_xy_scatter &lt;- function(df, subtitle = &quot;&quot;) { # extract first two simulations -------------------------------------------- df &lt;- df %&gt;% slice(1:2) %&gt;% unnest() # xy scatter plot function ------------------------------------------------- ggplot(df, aes(x, y, colour = id)) + geom_point(alpha = 0.3) + geom_abline(slope = 1, linetype = &quot;dashed&quot;) + labs( title = &quot;Response versus Explanatory Variable&quot;, subtitle = subtitle, x = &quot;x&quot;, y = &quot;y&quot;, colour = &#39;Simulation&#39; ) } # x order scatter plot function ------------------------------------------------ gg_sims_xorder_scatter &lt;- function(df, subtitle = &quot;&quot;) { # extract first two simulations -------------------------------------------- df &lt;- df %&gt;% slice(1:2) %&gt;% unnest() %&gt;% filter(between(t, no_obs/2 - 50, no_obs/2 + 50)) ggplot(df, aes(y = x, x = t, colour = id)) + geom_point(alpha = 0.8) + geom_line() + facet_grid(id ~ .) + labs( title = &quot;Explanatory Variable vesus Order of Observation&quot;, subtitle = subtitle, x = &quot;Order&quot;, y = &quot;x&quot;, colour = &#39;Simulation&#39; ) } # plot simulations -------------------------------------------------------------- gg_sims_xy_scatter(sims_xs, &quot;By observation, a linear model seems suitable&quot;) gg_sims_xorder_scatter(sims_xs, &quot;The order of observations is random&quot;) Figure 2.1: Scatter plots of cross sectional sample simulation defined by equation (2.1). The plots indicate that a lot of the the variance in \\(y\\) is explained by \\(x\\). The observations appear to be randomly scattered around the line, \\(y=x\\). Also, it is shown that there are no patterns in the order of observed \\(x\\). This is what is expected in a cross-sectional sample. Of course, this information is known since this it is a simulation. In practice, though, these plots are useful for exploring the characteristics of the data. These have an important impact on the performance of the model which will be discussed further in §3 and §4. 2.3.2 Time Series Simulation In order to simulate the effect of observing \\(x\\) and \\(y\\) from a time series, an adjustment is made to the simulation of cross-sectional data observations defined in equation (2.1). The time series simulation will be defined as: \\[ \\begin{align} y &amp;= x + \\eta_t \\\\ \\eta_t &amp; = \\phi_1 \\eta_{t-1} + e \\end{align} \\tag{2.2} \\] where \\(\\phi_1=0.5\\). The term, \\(\\eta_t\\), is an error term which is a modelled as first order auto-regressive (AR1) variable and is one of a family of models referred to as ARIMA models. These are commonly used to characterise patterns in regular, periodic time series modelling. A detailed discussion is out of the scope of this report and the reader may refer to Rob J. Hyndman and Athanasopoulos (2014), ch. 8. In short, in the AR1 model, the error level, \\(\\eta\\), is modelled by a regression with itself lagged by 1. Hence, the error term is linearly dependent on its own history. Similarly to the linear model, the term, \\(e\\), is a normal iid error. The explanatory variable, \\(x\\), is modelled as a sequence of numbers from -10 to 10, increasing at a constant rate, with an AR1 adjustment component. The model for \\(\\phi_1=0.5\\) is generated as follows: # function to generate simulation ---------------------------------------------- sim_ts_obs &lt;- function() { tibble( # time step ------------------------------------------------------------- t = 1:no_obs, # explanatory variable -------------------------------------------------- x = seq(-10, 10, length.out = no_obs) + arima.sim(model = list(ar = 0.5), n = no_obs, sd = sqrt(0.5)), # response -------------------------------------------------------------- y = x + arima.sim(model = list(ar = 0.5), n = no_obs, sd = sqrt(0.5)) ) } # repeat simulation many times ------------------------------------------------- sims_ts &lt;- tibble( id = map_chr(1:no_sims, make_id), obs = replicate(no_sims, sim_ts_obs(), simplify = FALSE)) Consider scatter plots of \\(y\\) versus \\(x\\) and \\(x\\) versus order for the middle 100 observations of the first two simulations. # plot simulations ------------------------------------------------------------- gg_sims_xy_scatter(sims_ts, &quot;By observation, a linear model seems suitable&quot;) gg_sims_xorder_scatter(sims_ts, &quot;The order of observations is not random&quot;) Figure 2.2: Scatter plots of time series sample simulation defined by equation (2.2). Compare figures 2.1 and 2.2, noting that the \\(x\\) axis in the bottom plot are not on the same scale. There is a clear difference in the characteristic of the cross-sectional and time series sample. Both quite clearly exhibit a linear relationship between \\(y\\) and \\(x\\); this is emphasised by the black dashed line of \\(y = x\\). However, contrary to the cross-sectional sample data, the observations for the time series sample are not independent from each other. In the time series, each observation is dependent on the preceding observations. There is a clear overall upward trend in \\(x\\) with time. Since \\(y\\) is linear dependent on \\(x\\), it also has an upward trend. Also there are superimposed local rising and falling trends. This latter feature is due to the auto-regression. The nature of the simulation is very similar to real observations of time series data. The impact of dependence between observations will be explored in §3 and §4. 2.4 Generic Modelling Function Each of the 10000 simulations represents a set of 500 observations that could have been sampled by an analyst. The R programming language provides many functions and packages which can be used to apply models to data. In this study, two models will be applied: a linear regression and ARIMAX. In the next block of code, a function is defined which will iterate through each simulation and apply a model of the users choice. apply_model &lt;- function(df, model, packages = NULL) { # set up cluster with one less core than is avaliable ---------------------- # note: must register packages and user defined functions my_cluster &lt;- create_cluster() %&gt;% cluster_library(c(&#39;tidyverse&#39;, &#39;broom&#39;, packages)) %&gt;% cluster_assign_value(&#39;model&#39;, model) %&gt;% set_default_cluster() # compute linear model ----------------------------------------------------- # extract coefficients and confidence intervals for x as tidy data frames df %&gt;% partition(cluster = my_cluster) %&gt;% mutate(mod = map(obs, model), coef = map(mod, tidy), conf_int = map(mod, confint_tidy)) %&gt;% collect() %&gt;% ungroup() %&gt;% arrange(id) } This function has been developed since it will be reused several times in this study. The above code block is worthy of some explanation: The arguments parsed to apply_model are the data frame of simulations, a function to apply the model and any packages required for the model function. The model function must take one argument which is the data frame of observations. These are nested within the simulation data frame in the column titled obs. The first block of code in the function initiates multi-core processing. partition splits the data frame for allocation to each core and collect collects the results together. These functions are from the multidplyr package which facilitates parallel analysis across multiple cores and significant time savings are achieved compared to serial computation. map is one of a family of functions from the purrr package for iterating functions across list elements. In this case, the model function is applied to each of observations nested in the simulation data frame. The tidy function from the broom package, is a wrapper around base R’s summary.lm function that returns a tidy data frame summarising the results of the model. confint_tidy is a similar wrapper around base R’s confint function which returns confidence intervals as a tidy data frame. The ungroup/arrange tidy up the data frame after being randomly split into groups for allocation to the cores for parallel processing. References "],
["results.html", "3 Results 3.1 Linear Regression Modelling 3.2 ARIMAX Modelling", " 3 Results 3.1 Linear Regression Modelling 3.1.1 Cross-Sectional Sample A linear regression model is implemented with the lm function from base R. In the next block of code, a function is defined to apply a linear model to each simulation and then the apply_model function, defined in §2.4, is invoked to apply the model to each of the simulated observations. # function for linear model ---------------------------------------------------- lm_mod &lt;- function(df) { lm(formula = y ~ x, data = df) } sims_xs_lm_fit &lt;- apply_model(sims_xs, model = lm_mod) The next block of code defines and invokes a function to plot the response, \\(y\\), versus the explanatory variable, \\(x\\), as a scatter plot and the linear regression model fitted with a black dashed line. The first 5 simulations are shown. gg_fit_xy_scatter &lt;- function(df, int_name, term_name, n = 5, subtitle = &quot;&quot;) { # extract first n simulations ---------------------------------------------- df &lt;- df %&gt;% slice(1:n) df_obs &lt;- df %&gt;% unnest(obs) df_coef &lt;- df %&gt;% unnest(coef) %&gt;% select(id, term, estimate) %&gt;% spread(key = term, estimate) %&gt;% rename_(.dots = setNames(list(int_name, term_name), list(&quot;a&quot;, &quot;b&quot;))) # y versus x with model fit plot function ---------------------------------- ggplot(df_obs, aes(x, y, colour = id)) + geom_point(alpha = 0.3) + geom_abline(data = df_coef, aes(slope = b, intercept = a), linetype = &#39;dashed&#39;, colour = &#39;black&#39;) + facet_grid(id ~ .) + labs( title = &quot;Response versus Explanatory Variable&quot;, subtitle = subtitle, x = &quot;Order&quot;, y = &quot;x&quot;, colour = &#39;Simulation&#39; ) } gg_fit_xy_scatter(sims_xs_lm_fit, int_name = &quot;`(Intercept)`&quot;, term_name = &quot;x&quot;, subtitle = &quot;Cross-sectional sample with Linear Model Fit&quot;) Figure 3.1: Scatter plots of the linear model applied to the cross sectional sample. In the next chunk of code, first, a function is defined which extracts the expected value and confidence intervals for the parameter \\(x\\) in each simulation. A variable, in_bound is defined which classifies each simulation based on whether the confidence interval includes the known population parameter for the \\(x\\) gradient i.e. 1. Next, a function is defined which generates table 3.1, showing the first 5 estimates of the coefficient of \\(x\\) and the corresponding confidence intervals for the linear model applied to the cross-sectional sample. # get each models expected value and confidence interval and categorise # each model by whether the confidence interval includes the known population # coefficient, 1. get_model_params &lt;- function(df, term_name) { df %&gt;% unnest(coef, conf_int) %&gt;% filter_(~term == term_name) %&gt;% mutate(y = as.factor(row_number(id)), in_bound = ifelse(conf.low &gt; 1 | conf.high &lt; 1, FALSE, TRUE)) } sims_xs_fit_lm_params &lt;- get_model_params(sims_xs_lm_fit, term_name = &quot;x&quot;) # create table of first n estimates and confidence intervals ------------------- kable_first_n_est_ci &lt;- function(df, n = 5, ...) { df %&gt;% slice(1:n) %&gt;% select(ID = id, Estimate = estimate, `95% CI Lower Bound` = conf.low, `95% CI Upper Bound` = conf.high) %&gt;% knitr::kable(..., booktabs = TRUE) } my_caption &lt;- paste( &quot;First five estimates and confidence intervals for the linear model&quot;, &quot;applied to the simulated cross-sectional sample described by equation&quot;, &quot;\\\\(2.1)&quot;) kable_first_n_est_ci(sims_xs_fit_lm_params, caption = my_caption, digits = 5) Table 3.1: First five estimates and confidence intervals for the linear model applied to the simulated cross-sectional sample described by equation (2.1) ID Estimate 95% CI Lower Bound 95% CI Upper Bound sim_001 0.99827 0.98337 1.01318 sim_002 0.99709 0.98182 1.01235 sim_003 0.98368 0.96836 0.99900 sim_004 0.99118 0.97573 1.00664 sim_005 1.00075 0.98515 1.01635 A useful model predicts the population characteristics given a sample of observations. Indeed, this is demonstrated in figure 3.1 and table 3.1. In the plots, the lines from the model fit are very similar to \\(y = x\\). The gradient is not exactly one since the irreducible error, \\(e\\), is random and this will influence the fit. In table 3.1, the quantities for gradient estimates for the first 5 simulations are given. The estimate is more completely described as: Estimate. An estimate of the expected value of the coefficient of x i.e. the value of \\(x\\) which is most likely given the sample observed. The definition of the 95 % confidence interval in the context of the estimated expected value of the coefficient is: Confidence Interval. If samples of the same size are repeatedly taken from a population which adheres to the model, \\(y = x\\), and a linear model fitted, the 95% confidence interval would contain the actual expected value of the coefficient of \\(x\\), i.e. 1, 95% of the time. The confidence interval (CI) quantifies the probable range of the coefficient of \\(x\\). For predictive analysis, it is important to compute the CI and to be confident that it is correct. It is observed from the figure and table that the estimates of the first 5 simulations are all close to 1 and that 4 out of 5 confidence intervals include the actual population parameter for the gradient of \\(x\\). The next block of code offers a visual demonstration of the meaning of the CI. gg_fit_ci &lt;- function(df, n = 40) { # extract first n simulations ---------------------------------------------- df &lt;- df %&gt;% slice(1:n) # plot confidence intervals as lines --------------------------------------- ggplot(df, aes(estimate, y)) + geom_point( aes(colour = factor(in_bound, levels = c(&quot;TRUE&quot;, &quot;FALSE&quot;)))) + geom_segment(aes(x = conf.low, xend = conf.high, y = y, yend = y, colour = in_bound), alpha = 0.5) + scale_colour_manual( values = c(&quot;FALSE&quot; = &#39;red&#39;, &quot;TRUE&quot; = &#39;black&#39;), drop = FALSE) + coord_cartesian(xlim = c(0.95, 1.05)) + geom_vline(xintercept = 1, colour = &#39;blue&#39;, linetype = &#39;dashed&#39;) + labs( title = &quot;Confidence intervals ranges for x coefficients of x&quot;, subtitle = &quot;Red CIs do NOT include the known coefficient of 1&quot;, x = &quot;x&quot;, y = &quot;Simulation No.&quot;, colour = &quot;CI includes 1?&quot; ) } gg_fit_ci(sims_xs_fit_lm_params) Figure 3.2: Line plots of confidence intervals for the linear model applied to the cross sectional sample. By definition, the expectation is that 2 out of 40 simulations will have a 95% CI that does not contain the actual population parameter. Indeed, this is observed, although it would not have been a surprise if this did not quite work out. After all, 40 simulations is a small sample and the analysis is stochastic. If the entire 10000 simulations are considered, the sample size of the samples is sufficient to verify the performance of the model against the known population parameters. In the following chunk of code, a function is created to report the average of the estimated expected values and the percentage of confidence intervals that contain the population coefficient of x. # function to report quality of fit -------------------------------------------- sim_report &lt;- function(df) { my_str = str_wrap(str_c( &quot;For %i simulations, with %i obsersvations in each simulation, the mean&quot;, &quot;expected value of the $x$ coefficient is %.3f and %.3f%% of the&quot;, &quot;simulations have 95%% CIs that contain the true value of $x$.&quot;, sep = &quot; &quot;), width = 70) sprintf( my_str, no_sims, no_obs, mean(df$estimate), 100 * mean(df$in_bound) ) } report_xs_fit_lm &lt;- sim_report(sims_xs_fit_lm_params) The output of the report is: For 10000 simulations, with 500 obsersvations in each simulation, the mean expected value of the \\(x\\) coefficient is 1.000 and 94.980% of the simulations have 95% CIs that contain the true value of \\(x\\). Recalling the definitions given in §2.3, the results indicate that when applying a linear regression to the cross-sectional sample simulations, that the model performance is: Unbiased - the mean of the estimate for the expected value of the \\(x\\) coefficient is extremely close to the actual population parameter. Efficient - the 95% CI includes the actual population parameter almost exactly 95% of the time. 3.1.2 Time Series Simulation Now, the time series simulations are considered. Recalling the plot for the time series simulations of response versus explanatory variable in figure 2.2, at first glance it may seem that applying a linear model would be reasonable. Observation suggests that the variance in \\(y\\) can be mostly explained by the variance in \\(x\\) and that the relationship is linear. In the next code block, similarly to in §2.3.1, a linear model is applied to the time series simulations. sims_ts_lm_fit &lt;- apply_model(sims_ts, lm_mod) Consider the scatter plot of the first 5 simulated observations (points) and the model fit (black line) shown in figure 3.3. gg_fit_xy_scatter(sims_ts_lm_fit, int_name = &quot;`(Intercept)`&quot;, term_name = &quot;x&quot;, subtitle = &quot;Time series sample with Linear Model Fit&quot;) Figure 3.3: Scatter plots of the linear model applied to the time series sample. Indeed, at a first glance, the fit looks to be very reasonable. However, consider now the plots of confidence intervals for the expected value of the gradient of \\(x\\). sims_ts_fit_lm_params &lt;- get_model_params(sims_ts_lm_fit, term_name = &quot;x&quot;) gg_fit_ci(sims_ts_fit_lm_params) Figure 3.4: Line plots of confidence intervals for the linear model applied to the time series sample. Recall that for 40 simulations, if the confidence interval is correct, statistically, only 2 should not include the known population parameter for the gradient i.e. 1. In figure 3.4, 17 confidence intervals do not include 1! Considering the each of the no_sims simulations using the sim_report function defined previously: report_ts_fit_lm &lt;- sim_report(sims_ts_fit_lm_params) For 10000 simulations, with 500 obsersvations in each simulation, the mean expected value of the \\(x\\) coefficient is 1.000 and 74.730% of the simulations have 95% CIs that contain the true value of \\(x\\). Hence, the finding is that if a linear regression is applied to a time series sample, that the model performance is: Unbiased - similarly to fitting a linear model to a cross-sectional sample, the mean of the estimate for the expected value of the \\(x\\) coefficient is extremely close to the actual population parameter. Inefficient - contrary to fitting a linear model to a cross-sectional sample, the 95% CI is incorrect and is too conservative. The reason for this inefficiency is discussed in 4. In short, the issue is that by definition, equation (2.2) contains two components: A linear relationship An auto-regressive relationship The linear model does not take into account the latter, hence, there is information which explains the observed response which is absent from the model. 3.2 ARIMAX Modelling 3.2.1 Fixed First Order Auto-regressive Error An approach to modelling regression with time series is described in Rob J. Hyndman and Athanasopoulos (2014), §9.1. The approach is referred to by various names and in this report, it will be referred to as ARIMAX (Auto-regression Integrated Moving Average with Explanatory Variables). The details are out of scope of this report but a brief description is useful. The approach involves fitting a linear regression and then fitting the residuals with an ARIMA model. In the simulations being described in this report, the order of the ARIMA model is fixed to be first order auto-regressive. In real cases, the order of the ARIMA model is unknown, so the approach involves some iteration until the residual after applying the ARIMA model is as close to white noise as is practically possible. The ARIMAX model is implemented in R with the forecast package (Rob J Hyndman 2017). First, the application and performance of the model will be demonstrated for a fixed first order auto-regressive error component: arimax_ar1_mod &lt;- function(df) { Arima(y = df$y, order = c(1, 0, 0), xreg = df$x) } sims_ts_arimax_ar1_fit &lt;- apply_model(sims_ts, arimax_ar1_mod, &#39;forecast&#39;) Figure 3.5 shows a scatter plot of the observations and the model fit for the first 5 simulations. gg_fit_xy_scatter(sims_ts_arimax_ar1_fit, int_name = &quot;intercept&quot;, term_name = &quot;`df$x`&quot;, subtitle = &quot;Time series sample with AR1 ARIMAX fit&quot;) Figure 3.5: Scatter plots of the AR1 ARIMAX model applied to the time series sample. By observation, it seems that the fit is quite reasonable. Now, consider the confidence intervals shown in figure 3.6: sims_ts_fit_arimax_ar1_params &lt;- get_model_params(sims_ts_arimax_ar1_fit, term_name = &quot;df$x&quot;) gg_fit_ci(sims_ts_fit_arimax_ar1_params) Figure 3.6: Line plots of confidence intervals for the linear model applied to the time series sample. Recalling that if the CI is correct, that 2 in 40 should not include the value 1. Hence, on this evidence, the ARIMAX model shows a significant improvement compared to CI results shown in figure 3.4 for the simple linear regression model. As before, the performance will be computed based on each of the no_sims simulations: report_ts_fit_arimax_ar1 &lt;- sim_report(sims_ts_fit_arimax_ar1_params) For 10000 simulations, with 500 obsersvations in each simulation, the mean expected value of the \\(x\\) coefficient is 1.000 and 94.540% of the simulations have 95% CIs that contain the true value of \\(x\\). This demonstrates that the ARIMAX model applied to the time series is: Unbiased - the mean of the estimate for the expected value of the \\(x\\) coefficient is extremely close to the actual population parameter. Efficient - the 95% CI includes the actual population parameter almost exactly 95% of the time. 3.2.2 Automatically Calculated Auto-regressive Error In the ARIMAX model described in §3.2.1, the error was fixed to have an order 1 auto-regressive error. As was discussed, in reality, it is very unlikely that the analysis will know the order of the error. For completeness, the next model uses an algorithm which iteratively seeks the best fit ARIMA error structure. It should be noted that automatic algorithms should be used with care and in particular, various diagnostic checks should be performed to ensure the models validity. This is out of scope and here, the automatic model will simply be applied and reported. The model is programmed as follows: arimax_auto_mod &lt;- function(df) { auto.arima(y = df$y, xreg = df$x) } sims_ts_arimax_auto_fit &lt;- apply_model(sims_ts, arimax_auto_mod, &#39;forecast&#39;) The performance is calculated as follows: sims_ts_fit_arimax_auto_params &lt;- get_model_params(sims_ts_arimax_auto_fit, term_name = &quot;df$x&quot;) ##&gt; Warning in bind_rows_(x, .id): Unequal factor levels: coercing to character report_ts_fit_arimax_auto &lt;- sim_report(sims_ts_fit_arimax_auto_params) It is found that the model is not biased but is a little inefficient compared to the model for fixed error structure. For 10000 simulations, with 500 obsersvations in each simulation, the mean expected value of the \\(x\\) coefficient is 1.000 and 94.082% of the simulations have 95% CIs that contain the true value of \\(x\\). References "],
["discussion.html", "4 Discussion", " 4 Discussion Table 4.1 summarises the results from the simulation. my_df &lt;- tibble( `Description` = c( &quot;Linear Model applied to Cross-Sectional Simulation&quot;, &quot;Linear Model applied to Time Series Simulation&quot;, &quot;AR1 ARIMAX Model applied to Time Series Simulation&quot;, &quot;Automatic ARIMAX Model applied to Time Series Simulation&quot; ), `Mean of Estimated Coefficient x` = c( mean(sims_xs_fit_lm_params$estimate), mean(sims_ts_fit_lm_params$estimate), mean(sims_ts_fit_arimax_ar1_params$estimate), mean(sims_ts_fit_arimax_auto_params$estimate) ), `% of 95% CI Containing Actual Coefficient x` = c( mean(100 * sims_xs_fit_lm_params$in_bound), mean(100 * sims_ts_fit_lm_params$in_bound), mean(100 * sims_ts_fit_arimax_ar1_params$in_bound), mean(100 * sims_ts_fit_arimax_auto_params$in_bound) ) ) knitr::kable(my_df, caption = &quot;Summary of Results&quot;) Table 4.1: Summary of Results Description Mean of Estimated Coefficient x % of 95% CI Containing Actual Coefficient x Linear Model applied to Cross-Sectional Simulation 1.0000899 94.9800 Linear Model applied to Time Series Simulation 0.9999779 74.7300 AR1 ARIMAX Model applied to Time Series Simulation 0.9999866 94.5400 Automatic ARIMAX Model applied to Time Series Simulation 0.9999722 94.0818 In §4.8 of Rob J. Hyndman and Athanasopoulos (2014), it is explained that if a linear regression is applied to time series samples, although the coefficient estimates model are unbiased, they are inefficient, which means that the variance of the estimates is higher than that which would be the case if the there was no auto-regression. This is what is observed if the confidence interval plots are studied i.e. compare 3.2 to figure 3.4. Notice how that although the \\(x\\) coefficient is known to be 1 in both cases, that the variance of the estimated expected values of \\(x\\) is greater for the time series simulation. This is what causes the CIs to be incorrect. When applying a regression model, the validity of the estimates and confidence intervals is conditional upon: The relationship between the explanatory variable and the response variable being linear. The residuals being independent from one another. The residuals having constant variance (heteroscedastic). The residuals being (nearly) normal. If these conditions do not hold, one of the impacts is that statistical inference, including the calculation of confidence intervals, will be incorrect. In §3, the linearity of the relationship was confirmed by observation of the data in figures 2.1 and 2.2. Turning attention to the residuals, it is informative to plot the residuals in order and to plot the auto-correlation function (ACF/PACF). Below, the function forecast::ggtsdisplay is used to produce these plots for: The linear model applied to the cross-sectional sample in figure 4.1 The linear model applied to the time series sample in figure 4.2 The AR1 ARIMAX model applied to the time series sample in figure 4.3 Note that only the residual for the first model in each simulation is shown. gg_fit_resids_tsdisplay &lt;- function(df) { df %&gt;% unnest(obs, resid) %&gt;% filter( id == &quot;sim_001&quot;, between(t, no_obs/2 - 100, no_obs/2 + 100)) %&gt;% select(resid) %&gt;% ts() %&gt;% ggtsdisplay(main = &quot;ACF plot&quot;) } ljung_box_test &lt;- function(df, fitdf = fitdf) { Box.test(df, lag = 10, fitdf = fitdf, type = &quot;Ljung&quot;)$p.value } sims_xs_lm_fit_resids &lt;- sims_xs_lm_fit %&gt;% mutate(resid = map(mod, resid)) %T&gt;% gg_fit_resids_tsdisplay() %&gt;% mutate(ljung_box_test = map_dbl(resid, ljung_box_test, fitdf = 0)) Figure 4.1: ACF Plot for Residuals for the linear model applied to the cross-sectional sample. sims_ts_lm_fit_resids &lt;- sims_ts_lm_fit %&gt;% mutate(resid = map(mod, resid)) %T&gt;% gg_fit_resids_tsdisplay() %&gt;% mutate(ljung_box_test = map_dbl(resid, ljung_box_test, fitdf = 0)) Figure 4.2: ACF Plot for Residuals for the linear model applied to the time series sample. sims_ts_arimax_ar1_fit_resids &lt;- sims_ts_arimax_ar1_fit %&gt;% mutate(resid = map(mod, resid)) %T&gt;% gg_fit_resids_tsdisplay() %&gt;% mutate(ljung_box_test = map_dbl(resid, ljung_box_test, fitdf = 1)) Figure 4.3: ACF Plot for Residuals for the AR1 ARIMAX model applied to the time series sample. In the above plots, the time series plots gives an indication as to whether any pattern exists in the residuals. For example, if a residual is high or low, is it more likely that the succeeding one will also be high or low. The ACF plot shows lines which indicate the magnitude of the residual regressed against itself lagged by various amounts. If the residual does not exhibit auto-regression, most of the lines (95%) should be less than the critical values shown by the blue dashed line. This line indicates a 5% significance level. Also, PACF plots are shown. The interpretation of ACF/PACF plots are described in §8.5 of Rob J. Hyndman and Athanasopoulos (2014). The residuals for the linear model applied to the cross-sectional sample (figure 4.1) show only one slightly significant peak at lag 17. Intuitively, a significant peak at lag 17 is likely to be due to sampling chance rather than being representative of a real population effect. Note that for 20 lags, a white noise sample will be expected to show 1 significant peak, since the significance level is 5%. In figure 4.2 which is for the linear model applied to the time series, one strong peak is observed at lag 1 and a second weaker one at lag 2 for the ACF plot. Also, a strong peak is observed at lag 1 for the PACF plot. This pattern suggests that there is auto-regression in the residuals. Finally, the plot in figure ?? for the ARIMAX model applied to the time series shows no evidence of remaining auto-regression in the residuals. Visual inspections can be supported by formal statistical tests. These are discussed in Rob J. Hyndman and Athanasopoulos (2014), §5.4. In the code block above, the Ljung-Box Test is applied to the residuals for each simulation run. This is a statistical test for the presence of auto-correlation. The null hypothesis is that there is no auto-correlation, therefore, it is expected that 95% of residuals that do not contain auto-regression will have p-value of more than 0.05. In the next code block, a table is generated to verify this. my_df &lt;- tibble( `Description` = c( &quot;Linear Model applied to Cross-Sectional Simulation&quot;, &quot;Linear Model applied to Time Series Simulation&quot;, &quot;AR1 ARIMAX Model applied to Time Series Simulation&quot; ), `Number of p-values &gt; 0.05 for Ljung Box Test` = c( mean(sims_xs_lm_fit_resids$ljung_box_test &gt; 0.05), mean(sims_ts_lm_fit_resids$ljung_box_test &gt; 0.05), mean(sims_ts_arimax_ar1_fit_resids$ljung_box_test &gt; 0.05) )) knitr::kable(my_df, caption = &quot;Summary of Ljung-Box Tests for Auto-regression Remaining in Residuals.&quot;) Table 4.2: Summary of Ljung-Box Tests for Auto-regression Remaining in Residuals. Description Number of p-values &gt; 0.05 for Ljung Box Test Linear Model applied to Cross-Sectional Simulation 0.9461 Linear Model applied to Time Series Simulation 0.0000 AR1 ARIMAX Model applied to Time Series Simulation 0.9457 The results shown in table 4.2 demonstrates that the Ljung-Box test performs as expected. The test selects the null hypothesis at the 5% significance level in 95% of cases when applied to the residuals for the linear model applied to the cross-sectional sample or the ARIMAX model applied to the time series simulations. However, when applying the linear model to the time series simulations, the Ljung-Box test selects the alternative hypothesis (i.e. that there is auto-correlation in the residuals) at the 5% significance level in every case. References "],
["conclusion.html", "5 Conclusion", " 5 Conclusion In this study, it has been confirmed, by simulation that statistical inference fails in the case that linear regression is applied to time series data. Although the coefficient estimates are unbiased, the confidence intervals are incorrect. It was shown that a much better model for regressing on time series is the ARIMAX model. In this case, the confidence intervals are accurate. "],
["references.html", "References", " References "]
]
